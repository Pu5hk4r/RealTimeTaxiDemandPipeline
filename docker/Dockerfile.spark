# docker/Dockerfile.spark
FROM bitnami/spark:3.3

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && \
            apt-get install -y --no-install-recommends \
    python3-dev \
    build-essential \
    libpq-dev && \
    rm -rf /var/lib/apt/lists/*

# Install Python requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt && \
                                  pip install pyspark==3.3.0

# Add PostgreSQL JDBC driver (required for Spark to connect to Postgres)
ENV POSTGRES_JDBC_VERSION=42.2.18
RUN curl -o /opt/bitnami/spark/jars/postgresql-${POSTGRES_JDBC_VERSION}.jar \
    https://jdbc.postgresql.org/download/postgresql-${POSTGRES_JDBC_VERSION}.jar

# Copy project files
COPY data_processing/ /app/data_processing/
COPY utils/ /app/utils/
COPY storage/ /app/storage/

# Set environment variables
ENV PYTHONPATH=/app
ENV SPARK_HOME=/opt/bitnami/spark
ENV PATH=$PATH:$SPARK_HOME/bin
ENV PYSPARK_PYTHON=python3

# Fix permissions
RUN chmod +x /app/data_processing/spark_transform.py

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD netstat -an | grep 7077 > /dev/null; if [ 0 != $? ]; then exit 1; fi;

# Default command (can be overridden in compose)
CMD ["/bin/bash", "-c", "while true; do sleep 30; done"]