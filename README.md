
<p align="center">
  <img src="https://img.icons8.com/external-flaticons-lineal-color-flat-icons/64/000000/external-taxi-transportation-flaticons-lineal-color-flat-icons.png" width="60"/>
</p>

<h1 align="center">ğŸš– NYC Taxi Demand Prediction Pipeline</h1>

<p align="center">
  Real-time machine learning pipeline for predicting zone-wise taxi demand in NYC using <b>Kafka</b>, <b>Spark</b>, <b>PostgreSQL</b>, and <b>Streamlit</b>.
</p>

<p align="center">
  <img src="https://img.shields.io/badge/Apache_Kafka-231F20?style=for-the-badge&logo=apachekafka&logoColor=white"/>
  <img src="https://img.shields.io/badge/Apache_Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white"/>
  <img src="https://img.shields.io/badge/PostgreSQL-4169E1?style=for-the-badge&logo=postgresql&logoColor=white"/>
  <img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white"/>
  <img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white"/>
</p>

---

## ğŸŒŸ Overview

> A full-stack real-time data pipeline for **predicting taxi demand across NYC** neighborhoods.  
> Designed for live ingestion and historical replay, it helps visualize traffic hot zones and optimize fleet allocation.
> Automatic Orchestrate By Airflow Train model and predict at particular timr interval.

![model1](https://github.com/Pu5hk4r/taxi-demand-pipeline/blob/main/assets/model1.png)
![model2](https://github.com/Pu5hk4r/taxi-demand-pipeline/blob/main/assets/model2.png)
![model3](https://github.com/Pu5hk4r/taxi-demand-pipeline/blob/main/assets/model3.png)

---

## ğŸ“Œ Table of Contents

- [ğŸ§  Project Motivation](#-project-motivation)
- [ğŸ› ï¸ Tools & Technologies](#ï¸-tools--technologies)
- [ğŸ“ System Architecture](#-system-architecture)
- [ğŸ“‚ Folder Structure](#-folder-structure)
- [ğŸ” Data Flow Pipeline](#-data-flow-pipeline)
- [ğŸ§ª ML Model Details](#-ml-model-details)
- [ğŸ“Š Live Dashboard](#-live-dashboard)
- [ğŸ—„ï¸ PostgreSQL Schema](#ï¸-postgresql-schema)
- [ğŸš€ Getting Started](#-getting-started)
- [ğŸ¯ Future Enhancements](#-future-enhancements)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“œ License](#-license)

---

## ğŸ§  Project Motivation

Urban areas suffer from:
- â±ï¸ Passenger wait times
- ğŸš– Idle driver time
- ğŸ’¸ Resource wastage

This project helps:
- ğŸ§­ Predict demand hotzones
- ğŸš¦ Optimize driver allocation
- ğŸ” Enable smarter transport systems

---

## ğŸ› ï¸ Tools & Technologies

| Layer        | Tool                      | Purpose                                      |
|--------------|----------------------------|----------------------------------------------|
| Ingestion    | Kafka                      | Real-time data transport                     |
| Processing   | Spark Structured Streaming | ETL, windowing, transformations              |
| ML           | scikit-learn               | Demand prediction using Random Forest        |
| Storage      | PostgreSQL                 | Store predictions                            |
| Dashboard    | Streamlit                  | Visual live updates                          |
| Orchestration| Airflow (MWAA)             | DAG scheduling (optional)                    |

---

## ğŸ“ System Architecture

![architecture](https://github.com/Pu5hk4r/taxi-demand-pipeline/blob/main/assets/PushkarProjectArchitecture.png)


graph TD
    A[Kafka Producer - NYC Taxi Trips] --> B[Kafka Topic: taxi_data]
    B --> C[Spark Structured Streaming]
    C --> D[Feature Engineering and Aggregation]
    D --> E["Trained ML Model - Random Forest"]
    E --> F[Kafka Topic: demand_prediction]
    F --> G1[Streamlit Dashboard]
    F --> G2[PostgreSQL Storage]

---
## ğŸ“‚ Folder Structure

<pre><code>ğŸ“¦ taxi-demand-prediction/ 

  taxi-demand-pipeline/
â”œâ”€â”€ kafka/                 # Kafka producer
â”‚   â””â”€â”€ producer.py
â”œâ”€â”€ spark/                 # PySpark streaming + transform
â”‚   â””â”€â”€ streaming_job.py
â”œâ”€â”€ models/                # Trained ML models (pkl/joblib)
â”‚   â””â”€â”€ demand_model.pkl
â”œâ”€â”€ airflow/               # DAGs for retraining/refresh
â”‚   â””â”€â”€ taxi_dag.py
â”œâ”€â”€ fastapi_app/           # Prediction API
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ predict.py
â”œâ”€â”€ streamlit_dashboard/   # Visualization UI
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ postgres/              # DB schema + queries
â”‚   â””â”€â”€ schema.sql
â”œâ”€â”€ docker/                # Docker Compose, Dockerfiles
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ data/                  # Sample CSV/JSON for local testing
â”œâ”€â”€ README.md
â””â”€â”€ .env

 </code></pre>



## ğŸ” Data Flow Pipeline

ğŸŸ¢ Kafka Producer streams data.

ğŸ”¶ Spark Streaming consumes and processes it.

ğŸ§® Feature Engineering on Spark.

ğŸ¤– ML Model predicts demand.

ğŸ“£ Predictions go to Kafka topic.

ğŸ“Š Streamlit shows demand.

ğŸ—ƒï¸ PostgreSQL stores for analysis.

---

## ğŸ§ª ML Model Details
Model: Random Forest Regressor

Framework: Scikit-learn

Features: Hour, Weekday, Zones, Distance, Passenger Count

Training Source: .parquet files

Metrics:

RÂ²: 0.91

RMSE: 2.65

---

## ğŸ“Š Live Dashboard
Built using Streamlit, with:

ğŸ—ºï¸ NYC zone-wise demand map

ğŸ”„ Auto-updates via Kafka

ğŸ”¢ Table + Graph modes

ğŸ¨ Color-coded predictions:

ğŸŸ¢ Low

ğŸŸ¡ Medium

ğŸ”´ High

---
## ğŸ—„ï¸ PostgreSQL Schema
sql
<pre><code>
  CREATE TABLE taxi_demand_prediction (
    id SERIAL PRIMARY KEY,
    zone_id INT,
    prediction_time TIMESTAMP,
    predicted_demand INT,
    model_version VARCHAR(50)
);

  
</code></pre>


---

## ğŸš€ Getting Started
## âœ… Step 1: Start Docker with Kafka + PGAdmin

<pre><code>
docker-compose -f docker/docker-compose.yml up -d
docker ps
</code></pre>
---
## âœ… Step 2: Prepare Database

<pre><code>
 psql -U postgres -c "CREATE DATABASE taxi_demand;"
psql -U postgres -d taxi_demand -f storage/schema.sql
psql -U postgres -d taxi_demand -f storage/sample_raw_trips.sql
</code></pre>
---

## âœ… Step 3: Process Data & Train Model

<pre><code>
python -m data_processing.spark_transform
python -m data_ingestion.kafka_producer
python -m ml_model.train_model
python -m ml_model.predict_demand

</code></pre>
---
## âœ… Step 4: Create Kafka Topics

<pre><code>
docker exec -it kafka kafka-topics --create --topic taxi_trips --bootstrap-server localhost:29092 --partitions 1 --replication-factor 1
docker exec -it kafka kafka-topics --create --topic taxi_zones --bootstrap-server localhost:29092 --partitions 1 --replication-factor 1
docker exec -it kafka kafka-topics --create --topic taxi_predictions --bootstrap-server localhost:29092 --partitions 1 --replication-factor 1

</code></pre>
---

## ğŸ¯ Future Enhancements
ğŸŒ¦ï¸ Add weather/traffic input

ğŸ“¦ Schema Registry + Kafka Connect

âš™ï¸ MLflow-based model tracking

ğŸ§Š Dockerized CI/CD deployment

ğŸ”” Spike alerts in Streamlit

---

## ğŸ¤ Contributing
Fork the repo

Create a branch

Submit a PR

Add your name to contributors ğŸ™Œ

---

## ğŸ“œ License
MIT License. See LICENSE.

<p align="center"> Built with â¤ï¸ by <strong>Pushkar Sharma</strong> </p> ```













